{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Garbage Image Detection project using Tensor Flow 2.0\n\n#### The model building process entails\n\n1. creating and training of a CNN(Convolutional Neural Network) using Tensor Flow 2.0\n2.Performing data processing and Augmentation \n3. Randomization\n4.Dealing with image datasets\n\n#### Prerequisite \npython programming language\n\nlinear and logistic regression(Bricks of neural networks)\n\nBasic understanding of image processing\n\nBasic understanding of Neural network and artificial neural networks.\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T19:29:28.24136Z","iopub.execute_input":"2021-08-24T19:29:28.241933Z","iopub.status.idle":"2021-08-24T19:29:34.746899Z","shell.execute_reply.started":"2021-08-24T19:29:28.241822Z","shell.execute_reply":"2021-08-24T19:29:34.745802Z"}}},{"cell_type":"markdown","source":"### Importing libraries","metadata":{}},{"cell_type":"code","source":"### Brief summary of the next steps of codes\nThe data would be loaded to path.\nThe function node takes an image path and it's labeled as arguments.\n\nFirst, we load the image font as an encoded string using the rete underscore file function provided\n\nby tenths of law.\n\nNext, we decode this encoded string using decode underscored JPEG function.\n\nthen we return the image Tensors and the label.\n\n\n\nFirst, we create our sequential object and we define transformations in it.\n\n\nThe resizing function is defined inside the layers API and preprocessing module.\n\nThis function takes in two arguments.\n\nFirst is the height, and second is that the initial size of the image in this dataset is 48 by 48,\n\nwhich is very small compared to the current deep learning standards.\n\nBut we also have to think about noise by upscaling it.\n\nHere we are upscaling it to ninety six, which is double in terms of the initial height and weight.\n\nNext, we have our data augmentation.\n\nWe again start a sequential object and define the augmentation methods in it.\n\nThe first augmentation is random flip.\n\nWe pass on horizontal as an argument to it, notifying it to rotate the image horizontally in a random\n\nfashion.\n\nNext, we have random rotate.\n\nNow this augmentation is used to rotate an image at a certain angle.\n\nNext, we have random zoom as well as the name suggests it is going to zoom in or zoom out of the image\n\nrandomly\n\nIf the argument is negative, the function would zoom in and if the argument is positive, it would\n\nzoom out.\n\nZooming in, the image would allow the model to lower the core features in the image effectively.\n\nSo according to our arguments, we are randomly zooming in five to 10 percent.\n\nNow that all of these augmentations are wrapped up in the sequential object, we can simply call this\n\nobject as a function and apply all these transformations in a sequence.\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install opencv-python --upgrade","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:20:45.782753Z","iopub.execute_input":"2021-09-14T13:20:45.783221Z","iopub.status.idle":"2021-09-14T13:21:00.626767Z","shell.execute_reply.started":"2021-09-14T13:20:45.783133Z","shell.execute_reply":"2021-09-14T13:21:00.625702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n# for data visualization and data wrangling \nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n## For reading files to path import\nimport os\nimport pathlib\nfrom pathlib import Path\n\n## For Modelling \nfrom tqdm import tqdm\nfrom keras.preprocessing import image\nimport zipfile\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport glob, os, random\n\n\ntf.random.set_seed(4)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:00.628602Z","iopub.execute_input":"2021-09-14T13:21:00.628987Z","iopub.status.idle":"2021-09-14T13:21:05.133703Z","shell.execute_reply.started":"2021-09-14T13:21:00.628933Z","shell.execute_reply":"2021-09-14T13:21:05.132661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Getting the dataset","metadata":{}},{"cell_type":"code","source":"print(os.listdir(\"../input/split-garbage-dataset\"))\n#print(os.listdir('../input/split-garbage-dataset/null'))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:05.13543Z","iopub.execute_input":"2021-09-14T13:21:05.135763Z","iopub.status.idle":"2021-09-14T13:21:05.151623Z","shell.execute_reply.started":"2021-09-14T13:21:05.135733Z","shell.execute_reply":"2021-09-14T13:21:05.150663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(os.listdir(\"../input/split-garbage-dataset\"))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:05.153261Z","iopub.execute_input":"2021-09-14T13:21:05.153609Z","iopub.status.idle":"2021-09-14T13:21:05.165046Z","shell.execute_reply.started":"2021-09-14T13:21:05.153576Z","shell.execute_reply":"2021-09-14T13:21:05.164283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Pathlib PATH objects\n\ntrain_path = Path(\"../input/split-garbage-dataset/train\")\ntest_path = Path(\"../input/split-garbage-dataset/test\")\nvalid_path = Path(\"../input/split-garbage-dataset/valid\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:05.166136Z","iopub.execute_input":"2021-09-14T13:21:05.166466Z","iopub.status.idle":"2021-09-14T13:21:05.171043Z","shell.execute_reply.started":"2021-09-14T13:21:05.166423Z","shell.execute_reply":"2021-09-14T13:21:05.17001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting Image paths and reading 10 items\n\ntrain_image_paths = list(train_path.glob(\"*/*\"))\ntrain_image_paths = list(map(lambda x : str(x) , train_image_paths))\n\ntrain_image_paths[:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:05.172354Z","iopub.execute_input":"2021-09-14T13:21:05.172745Z","iopub.status.idle":"2021-09-14T13:21:05.958453Z","shell.execute_reply.started":"2021-09-14T13:21:05.172707Z","shell.execute_reply":"2021-09-14T13:21:05.957677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for getting image respective labels using map funtion\n\ndef get_label(image_path):\n    return image_path.split(\"/\")[-2]\n\ntrain_image_labels = list(map(lambda x : get_label(x) , train_image_paths))\ntrain_image_labels[:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:05.959716Z","iopub.execute_input":"2021-09-14T13:21:05.960067Z","iopub.status.idle":"2021-09-14T13:21:05.970014Z","shell.execute_reply.started":"2021-09-14T13:21:05.960031Z","shell.execute_reply":"2021-09-14T13:21:05.968644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder \n\nEncoder = LabelEncoder()\ntrain_image_labels = Encoder.fit_transform(train_image_labels)\n\ntrain_image_labels[:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:05.973241Z","iopub.execute_input":"2021-09-14T13:21:05.973839Z","iopub.status.idle":"2021-09-14T13:21:06.463348Z","shell.execute_reply.started":"2021-09-14T13:21:05.973785Z","shell.execute_reply":"2021-09-14T13:21:06.46264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting one hot encoded values of unique label\n\ntrain_image_labels = tf.keras.utils.to_categorical(train_image_labels)\n\ntrain_image_labels[:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:06.465208Z","iopub.execute_input":"2021-09-14T13:21:06.465548Z","iopub.status.idle":"2021-09-14T13:21:06.473021Z","shell.execute_reply.started":"2021-09-14T13:21:06.465513Z","shell.execute_reply":"2021-09-14T13:21:06.472169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\nTrain_paths , Val_paths , Train_labels , Val_labels = train_test_split(train_image_paths , train_image_labels , test_size = 0.25)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:06.474225Z","iopub.execute_input":"2021-09-14T13:21:06.474769Z","iopub.status.idle":"2021-09-14T13:21:06.521244Z","shell.execute_reply.started":"2021-09-14T13:21:06.474724Z","shell.execute_reply":"2021-09-14T13:21:06.520516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class Balancing","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:23:59.181049Z","iopub.execute_input":"2021-08-25T16:23:59.181405Z","iopub.status.idle":"2021-08-25T16:23:59.186385Z","shell.execute_reply.started":"2021-08-25T16:23:59.18137Z","shell.execute_reply":"2021-08-25T16:23:59.185231Z"}}},{"cell_type":"markdown","source":"since we are training a multiclass classifier we need to take care of class inbalances.\nWe calculate the amount of weight that has to be given to each class. ","metadata":{}},{"cell_type":"code","source":"# Compute class weights \n\nclassTotals = Train_labels.sum(axis=0)\nclassWeight = classTotals.max() / classTotals\n\nclass_weight = {e : weight for e , weight in enumerate(classWeight)}\nprint(class_weight)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:06.522375Z","iopub.execute_input":"2021-09-14T13:21:06.522697Z","iopub.status.idle":"2021-09-14T13:21:06.530744Z","shell.execute_reply.started":"2021-09-14T13:21:06.522663Z","shell.execute_reply":"2021-09-14T13:21:06.52927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n    The weight of the classes are quiet close except for class 5 which contains plastic hence we see higher  weight getting assigned to plastic.","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:28:28.087017Z","iopub.execute_input":"2021-08-25T16:28:28.087373Z","iopub.status.idle":"2021-08-25T16:28:28.092379Z","shell.execute_reply.started":"2021-08-25T16:28:28.08734Z","shell.execute_reply":"2021-08-25T16:28:28.091099Z"}}},{"cell_type":"code","source":"# Function used for Transformation\n\ndef load(image , label):\n    image = tf.io.read_file(image)\n    image = tf.io.decode_jpeg(image , channels = 3)\n    return image , label","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:06.531844Z","iopub.execute_input":"2021-09-14T13:21:06.532082Z","iopub.status.idle":"2021-09-14T13:21:06.540021Z","shell.execute_reply.started":"2021-09-14T13:21:06.53206Z","shell.execute_reply":"2021-09-14T13:21:06.539101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Define image size  and batch size","metadata":{}},{"cell_type":"code","source":" \nIMG_SIZE = 96 \nBATCH_SIZE = 32\n\n# Basic Transformation\nresize = tf.keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.Resizing(IMG_SIZE, IMG_SIZE)          \n])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:06.541277Z","iopub.execute_input":"2021-09-14T13:21:06.541681Z","iopub.status.idle":"2021-09-14T13:21:08.525123Z","shell.execute_reply.started":"2021-09-14T13:21:06.541645Z","shell.execute_reply":"2021-09-14T13:21:08.524233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Augmentation\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n    tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor = (-0.1, -0.05))\n])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:08.526474Z","iopub.execute_input":"2021-09-14T13:21:08.52683Z","iopub.status.idle":"2021-09-14T13:21:08.550347Z","shell.execute_reply.started":"2021-09-14T13:21:08.526777Z","shell.execute_reply":"2021-09-14T13:21:08.549585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function used to Create a Tensorflow Data Object\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ndef get_dataset(paths , labels , train = True):\n    image_paths = tf.convert_to_tensor(paths)\n    labels = tf.convert_to_tensor(labels)\n\n    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n    label_dataset = tf.data.Dataset.from_tensor_slices(labels)\n\n    dataset = tf.data.Dataset.zip((image_dataset , label_dataset))\n\n    dataset = dataset.map(lambda image , label : load(image , label))\n    dataset = dataset.map(lambda image, label: (resize(image), label) , num_parallel_calls=AUTOTUNE)\n    dataset = dataset.shuffle(1000)\n    dataset = dataset.batch(BATCH_SIZE)\n\n    if train:\n        dataset = dataset.map(lambda image, label: (data_augmentation(image), label) , num_parallel_calls=AUTOTUNE)\n    \n    dataset = dataset.repeat()\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:08.551521Z","iopub.execute_input":"2021-09-14T13:21:08.551855Z","iopub.status.idle":"2021-09-14T13:21:08.560587Z","shell.execute_reply.started":"2021-09-14T13:21:08.551821Z","shell.execute_reply":"2021-09-14T13:21:08.559757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Train Dataset object and Verifying it\n%time train_dataset = get_dataset(Train_paths , Train_labels)\n\nimage , label = next(iter(train_dataset))\nprint(image.shape)\nprint(label.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:08.561979Z","iopub.execute_input":"2021-09-14T13:21:08.562359Z","iopub.status.idle":"2021-09-14T13:21:16.137616Z","shell.execute_reply.started":"2021-09-14T13:21:08.562322Z","shell.execute_reply":"2021-09-14T13:21:16.136733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View a sample Training Image\nprint(Encoder.inverse_transform(np.argmax(label , axis = 1))[0])\nplt.imshow((image[0].numpy()/255).reshape(96 , 96 , 3))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:16.139004Z","iopub.execute_input":"2021-09-14T13:21:16.139334Z","iopub.status.idle":"2021-09-14T13:21:16.311064Z","shell.execute_reply.started":"2021-09-14T13:21:16.139296Z","shell.execute_reply":"2021-09-14T13:21:16.310256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time val_dataset = get_dataset(Val_paths , Val_labels , train = False)\n\nimage , label = next(iter(val_dataset))\nprint(image.shape)\nprint(label.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:16.312343Z","iopub.execute_input":"2021-09-14T13:21:16.312687Z","iopub.status.idle":"2021-09-14T13:21:19.450651Z","shell.execute_reply.started":"2021-09-14T13:21:16.312653Z","shell.execute_reply":"2021-09-14T13:21:19.449567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View a sample Validation Image\nprint(Encoder.inverse_transform(np.argmax(label , axis = 1))[0])\nplt.imshow((image[0].numpy()/255).reshape(96 , 96 , 3))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:19.452112Z","iopub.execute_input":"2021-09-14T13:21:19.452461Z","iopub.status.idle":"2021-09-14T13:21:19.607605Z","shell.execute_reply.started":"2021-09-14T13:21:19.452423Z","shell.execute_reply":"2021-09-14T13:21:19.606618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building EfficientNet model ...Normalization is already included as part of the model\n\nfrom tensorflow.keras.applications import EfficientNetB2\n\nbackbone = EfficientNetB2(\n    input_shape=(96, 96, 3),\n    include_top=False\n)\n\nmodel = tf.keras.Sequential([\n    backbone,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(6, activation='softmax')\n])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:19.609092Z","iopub.execute_input":"2021-09-14T13:21:19.609488Z","iopub.status.idle":"2021-09-14T13:21:24.385698Z","shell.execute_reply.started":"2021-09-14T13:21:19.609427Z","shell.execute_reply":"2021-09-14T13:21:24.38491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compiling the model by providing the Optimizer , Loss and Metrics\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-06),\n    loss = 'categorical_crossentropy',\n    metrics=['accuracy' , tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')]\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:24.386967Z","iopub.execute_input":"2021-09-14T13:21:24.387302Z","iopub.status.idle":"2021-09-14T13:21:24.415778Z","shell.execute_reply.started":"2021-09-14T13:21:24.387266Z","shell.execute_reply":"2021-09-14T13:21:24.41502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    train_dataset,\n    steps_per_epoch=len(Train_paths)//BATCH_SIZE,\n    epochs=10,\n    validation_data=val_dataset,\n    validation_steps = len(Val_paths)//BATCH_SIZE,\n    class_weight=class_weight\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:21:24.416927Z","iopub.execute_input":"2021-09-14T13:21:24.417265Z","iopub.status.idle":"2021-09-14T13:22:51.027879Z","shell.execute_reply.started":"2021-09-14T13:21:24.417228Z","shell.execute_reply":"2021-09-14T13:22:51.027022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.layers[0].trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:22:51.033204Z","iopub.execute_input":"2021-09-14T13:22:51.033469Z","iopub.status.idle":"2021-09-14T13:22:51.049578Z","shell.execute_reply.started":"2021-09-14T13:22:51.033436Z","shell.execute_reply":"2021-09-14T13:22:51.048848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining our callbacks \ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\"best_weights.h5\",verbose=1,save_best_only=True,save_weights_only = True)\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=4)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:22:51.052585Z","iopub.execute_input":"2021-09-14T13:22:51.052845Z","iopub.status.idle":"2021-09-14T13:22:51.057398Z","shell.execute_reply.started":"2021-09-14T13:22:51.05282Z","shell.execute_reply":"2021-09-14T13:22:51.056106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:22:51.059059Z","iopub.execute_input":"2021-09-14T13:22:51.059696Z","iopub.status.idle":"2021-09-14T13:22:51.086674Z","shell.execute_reply.started":"2021-09-14T13:22:51.059659Z","shell.execute_reply":"2021-09-14T13:22:51.085656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    train_dataset,\n    steps_per_epoch=len(Train_paths)//BATCH_SIZE,\n    epochs=8,\n    callbacks=[checkpoint , early_stop],\n    validation_data=val_dataset,\n    validation_steps = len(Val_paths)//BATCH_SIZE,\n    class_weight=class_weight\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:22:51.087857Z","iopub.execute_input":"2021-09-14T13:22:51.08819Z","iopub.status.idle":"2021-09-14T13:23:30.840166Z","shell.execute_reply.started":"2021-09-14T13:22:51.088157Z","shell.execute_reply":"2021-09-14T13:23:30.839337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing the Model","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:32:36.780033Z","iopub.execute_input":"2021-08-23T21:32:36.780363Z","iopub.status.idle":"2021-08-23T21:32:37.054208Z","shell.execute_reply.started":"2021-08-23T21:32:36.780327Z","shell.execute_reply":"2021-08-23T21:32:37.053338Z"}}},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB2\n\nbackbone = EfficientNetB2(\n    input_shape=(96, 96, 3),\n    include_top=False\n)\n\nmodel = tf.keras.Sequential([\n    backbone,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(6, activation='softmax')\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-06),\n    loss = 'categorical_crossentropy',\n    metrics=['accuracy' , tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')]\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:30.841418Z","iopub.execute_input":"2021-09-14T13:23:30.841744Z","iopub.status.idle":"2021-09-14T13:23:33.977457Z","shell.execute_reply.started":"2021-09-14T13:23:30.841708Z","shell.execute_reply":"2021-09-14T13:23:33.976604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(\"best_weights.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:33.978601Z","iopub.execute_input":"2021-09-14T13:23:33.978954Z","iopub.status.idle":"2021-09-14T13:23:34.330428Z","shell.execute_reply.started":"2021-09-14T13:23:33.978922Z","shell.execute_reply":"2021-09-14T13:23:34.32949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Dataset Object for 'Testing' Set just the way we did for Training and Validation\ntest_image_paths = list(test_path.glob(\"*/*\"))\ntest_image_paths = list(map(lambda x : str(x) , test_image_paths))\ntest_labels = list(map(lambda x : get_label(x) , test_image_paths))\n\ntest_labels = Encoder.transform(test_labels)\ntest_labels = tf.keras.utils.to_categorical(test_labels)\n\ntest_image_paths = tf.convert_to_tensor(test_image_paths)\ntest_labels = tf.convert_to_tensor(test_labels)\n\ndef decode_image(image , label):\n    image = tf.io.read_file(image)\n    image = tf.io.decode_jpeg(image , channels = 3)\n    image = tf.image.resize(image , [96 , 96] , method=\"bilinear\")\n    return image , label\n\ntest_dataset = (\n     tf.data.Dataset\n    .from_tensor_slices((test_image_paths, test_labels))\n    .map(decode_image)\n    .batch(BATCH_SIZE)\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:34.33478Z","iopub.execute_input":"2021-09-14T13:23:34.336813Z","iopub.status.idle":"2021-09-14T13:23:34.621358Z","shell.execute_reply.started":"2021-09-14T13:23:34.336762Z","shell.execute_reply":"2021-09-14T13:23:34.620349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify Test Dataset Object\nimage , label = next(iter(test_dataset))\nprint(image.shape)\nprint(label.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:34.625531Z","iopub.execute_input":"2021-09-14T13:23:34.625887Z","iopub.status.idle":"2021-09-14T13:23:34.914339Z","shell.execute_reply.started":"2021-09-14T13:23:34.625853Z","shell.execute_reply":"2021-09-14T13:23:34.913501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View a sample Validation Image\nprint(Encoder.inverse_transform(np.argmax(label , axis = 1))[0])\nplt.imshow((image[0].numpy()/255).reshape(96 , 96 , 3))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:34.917618Z","iopub.execute_input":"2021-09-14T13:23:34.918178Z","iopub.status.idle":"2021-09-14T13:23:35.093283Z","shell.execute_reply.started":"2021-09-14T13:23:34.918141Z","shell.execute_reply":"2021-09-14T13:23:35.090576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluating the loaded model\nloss, acc, prec, rec = model.evaluate(test_dataset)\n\nprint(\" Testing Acc : \" , acc)\nprint(\" Testing Precision \" , prec)\nprint(\" Testing Recall \" , rec)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:35.094788Z","iopub.execute_input":"2021-09-14T13:23:35.095164Z","iopub.status.idle":"2021-09-14T13:23:41.472585Z","shell.execute_reply.started":"2021-09-14T13:23:35.095127Z","shell.execute_reply":"2021-09-14T13:23:41.471562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation: The Model have have acccuracy of over 84 percent, precision of about 85 percentand 83 percent recall.","metadata":{}},{"cell_type":"markdown","source":"### Saving the Model ","metadata":{}},{"cell_type":"code","source":"# Save Model\nmodel.save(\"GarbageImageclassifierModel.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:41.47395Z","iopub.execute_input":"2021-09-14T13:23:41.474308Z","iopub.status.idle":"2021-09-14T13:23:41.891009Z","shell.execute_reply.started":"2021-09-14T13:23:41.474272Z","shell.execute_reply":"2021-09-14T13:23:41.890151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save Label Encoder \nimport pickle\n\ndef save_object(obj , name):\n    pickle_obj = open(f\"{name}.pck\",\"wb\")\n    pickle.dump(obj, pickle_obj)\n    pickle_obj.close()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:41.892363Z","iopub.execute_input":"2021-09-14T13:23:41.892688Z","iopub.status.idle":"2021-09-14T13:23:41.898929Z","shell.execute_reply.started":"2021-09-14T13:23:41.892654Z","shell.execute_reply":"2021-09-14T13:23:41.896558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_object(Encoder,\"LabelEncoder\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:41.900369Z","iopub.execute_input":"2021-09-14T13:23:41.900733Z","iopub.status.idle":"2021-09-14T13:23:41.908348Z","shell.execute_reply.started":"2021-09-14T13:23:41.900697Z","shell.execute_reply":"2021-09-14T13:23:41.907473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For real time usage","metadata":{}},{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\n\nimport dlib\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:41.909591Z","iopub.execute_input":"2021-09-14T13:23:41.910156Z","iopub.status.idle":"2021-09-14T13:23:42.042322Z","shell.execute_reply.started":"2021-09-14T13:23:41.910119Z","shell.execute_reply":"2021-09-14T13:23:42.041518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    backbone = tf.keras.applications.EfficientNetB2(\n        input_shape=(96, 96, 3),\n        include_top=False,\n        weights=None\n    )\n    model = tf.keras.Sequential([\n        backbone,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(6, activation='softmax')\n    ])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:42.044108Z","iopub.execute_input":"2021-09-14T13:23:42.044349Z","iopub.status.idle":"2021-09-14T13:23:42.050463Z","shell.execute_reply.started":"2021-09-14T13:23:42.044325Z","shell.execute_reply":"2021-09-14T13:23:42.049452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.load_weights(\"best_weights.h5\") # Load the saved weights ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:42.0519Z","iopub.execute_input":"2021-09-14T13:23:42.052586Z","iopub.status.idle":"2021-09-14T13:23:45.280641Z","shell.execute_reply.started":"2021-09-14T13:23:42.052549Z","shell.execute_reply":"2021-09-14T13:23:45.279815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load LabelEncoder \ndef load_object(name):\n    pickle_obj = open(f\"{name}.pck\",\"rb\")\n    obj = pickle.load(pickle_obj)\n    return obj\n\nLe = load_object(\"LabelEncoder\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:45.281894Z","iopub.execute_input":"2021-09-14T13:23:45.282209Z","iopub.status.idle":"2021-09-14T13:23:45.289748Z","shell.execute_reply.started":"2021-09-14T13:23:45.282178Z","shell.execute_reply":"2021-09-14T13:23:45.288874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ProcessImage(image):\n    image = tf.convert_to_tensor(image)\n    image = tf.image.resize(image , [96 , 96] , method=\"bilinear\")\n    image = tf.expand_dims(image , 0)\n    return image\n\ndef RealtimePrediction(image , model, encoder_):\n    prediction = model.predict(image)\n    prediction = np.argmax(prediction , axis = 1)\n    return encoder_.inverse_transform(prediction)[0]\n\ndef rect_to_bb(rect):\n    x = rect.left()\n    y = rect.top()\n    w = rect.right() - x\n    h = rect.bottom() - y\n    return (x, y, w, h)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:45.290979Z","iopub.execute_input":"2021-09-14T13:23:45.291346Z","iopub.status.idle":"2021-09-14T13:23:45.298955Z","shell.execute_reply.started":"2021-09-14T13:23:45.29131Z","shell.execute_reply":"2021-09-14T13:23:45.29781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VideoCapture = cv2.VideoCapture(0)\n\ndetector = dlib.get_frontal_face_detector()\n\nwhile True :\n    \n    ret , frame = VideoCapture.read() \n    \n    if not ret :\n        break\n\n    gray = cv2.cvtColor( frame , cv2.COLOR_BGR2GRAY)\n\n    rects = detector(gray , 0)\n\n    if len(rects) >= 1 :\n        for rect in rects :\n            (x , y , w , h) = rect_to_bb(rect)\n            img = gray[y-10 : y+h+10 , x-10 : x+w+10]\n            \n            if img.shape[0] == 0 or img.shape[1] == 0 :\n                cv2.imshow(\"Frame\", frame)\n                \n            else :\n                img = cv2.cvtColor(img , cv2.COLOR_GRAY2RGB)\n                img = ProcessImage(img)\n                out = RealtimePrediction(img , model , Le)\n                cv2.rectangle(frame, (x, y), (x+w, y+h),(0, 255, 0), 2)\n                z = y - 15 if y - 15 > 15 else y + 15\n                cv2.putText(frame, str(out), (x, z), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n                \n        cv2.imshow(\"Frame\", frame)\n            \n    else :\n        cv2.imshow(\"Frame\", frame)\n        \n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n        \nVideoCapture.release()\ncv2.destroyAllWindows()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:23:45.300193Z","iopub.execute_input":"2021-09-14T13:23:45.300731Z","iopub.status.idle":"2021-09-14T13:23:45.825944Z","shell.execute_reply.started":"2021-09-14T13:23:45.300694Z","shell.execute_reply":"2021-09-14T13:23:45.825063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}